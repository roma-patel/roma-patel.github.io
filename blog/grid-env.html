

<!DOCTYPE html>
<html>
<title>Roma Patel</title>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<link rel="stylesheet" href="https://www.w3schools.com/w3css/4/w3.css">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
body,h1,h2,h3,h4,h5,h6 {font-family: "Montserrat", sans-serif}
.w3-bar,h1,button {font-family: "Montserrat", sans-serif}
.fa-anchor,.fa-coffee {font-size:200px}
a:link {
    color: green;
    background-color: transparent;
    text-decoration: none;
}
a:visited {
    color: green;
    background-color: transparent;
    text-decoration: none;
}
</style>
<body>

<!-- Navbar -->
<div class="w3-top">
  <div class="w3-bar w3-white w3-card w3-left-align w3-large">
    <a class="w3-bar-item w3-button w3-hide-medium w3-hide-large w3-right w3-padding-large w3-hover-white w3-large w3-white" href="javascript:void(0);" onclick="myFunction()" title="Toggle Navigation Menu"><i class="fa fa-bars"></i></a>
    <a href="#" class="w3-bar-item w3-button w3-padding-large w3-white"></a>
    <a href=/people/rpatel59/index.html class="w3-bar-item w3-button w3-hide-small w3-padding-large w3-hover-white">About</a>
    <a href=/people/rpatel59/research-code.html class="w3-bar-item w3-button w3-hide-small w3-padding-large w3-hover-white">Research+Code</a>
    <a href=/people/rpatel59/blog.html class="w3-bar-item w3-button w3-hide-small w3-padding-large w3-hover-white">Blog</a>
  </div>

  <!-- Navbar on small screens -->
  <div id="navDemo" class="w3-bar-block w3-white w3-hide w3-hide-large w3-hide-medium w3-large">
    <a href=/people/rpatel59/index.html class="w3-bar-item w3-button w3-padding-large">About</a>
    <a href=/people/rpatel59/research-code.html class="w3-bar-item w3-button w3-padding-large">Research+Code</a>
    <a href=/people/rpatel59/blog.html class="w3-bar-item w3-button w3-padding-large">Blog</a>
  </div>
</div>

<!-- Header -->
<header class="w3-container w3-black w3-center" style="padding:128px 16px">
</header>

<!-- First Grid -->
<div class="w3-row-padding w3-padding-64 w3-container">
  <div class="w3-content">
    <div class="w3-twothird">
      <h1>NLP Resources</h1>
      	<p>I came across this list a while ago, and it has been <i>hugely</i> useful to me, which is why I’ve augmented it and put it up here. It contains an (almost) exhaustive list of libraries, packages, tools, implementations of tasks, tutorials, current state of the art models and established baselines.</p>   
	<p></p>
	<p>The version that I saw first also had extensive video lectures, tutorials and article/paper references, which I cannot seem to find, but when I do, it will go under the first resource list. Enjoy!</p>
	<p></p>
	<p></p>
	<p></p>

	<p>Implementations:</p>
<ul>
<li><a href="https://github.com/ai-ku/wvec">Pre-trained word embeddings for WSJ corpus</a> by Koc AI-Lab</li>
<li><a href="https://code.google.com/archive/p/word2vec">Word2vec</a> by Mikolov</li>
<li><a href="http://metaoptimize.com/projects/wordreprs/">HLBL language model</a> by Turian</li>
<li><a href="http://www.cis.upenn.edu/%7Eungar/eigenwords/">Real-valued vector "embeddings"</a> by Dhillon</li>
<li><a href="http://www.socher.org/index.php/Main/ImprovingWordRepresentationsViaGlobalContextAndMultipleWordPrototypes">Improving Word Representations Via Global Context And Multiple Word Prototypes</a> by Huang</li>
<li><a href="https://levyomer.wordpress.com/2014/04/25/dependency-based-word-embeddings/">Dependency based word embeddings</a></li>
<li><a href="http://nlp.stanford.edu/projects/glove/">Global Vectors for Word Representations</a></li>
</ul>
	<p></p>
	<p></p>
	<p></p>

	<p>Libraries:</p
<ul>
<li>
<a href="http://www.anthology.aclweb.org/R/R13/R13-1011.pdf">TwitIE: An Open-Source Information Extraction Pipeline for Microblog Text</a>
</li>
<li>
<a id="user-content-node-js"><strong>Node.js and Javascript</strong> - Node.js Libaries for NLP</a>
<ul>
<li><a href="https://github.com/twitter/twitter-text">Twitter-text</a> - A JavaScript implementation of Twitter's text processing library</li>
<li><a href="https://github.com/loadfive/Knwl.js">Knwl.js</a> - A Natural Language Processor in JS</li>
<li><a href="https://github.com/wooorm/retext">Retext</a> - Extensible system for analyzing and manipulating natural language</li>
<li><a href="https://github.com/nlp-compromise/nlp_compromise">NLP Compromise</a> - Natural Language processing in the browser</li>
<li><a href="https://github.com/NaturalNode/natural">Natural</a> - general natural language facilities for node</li>
</ul>	
	<p></p>
	<p></p>
	<p></p>

	<p>NLP Python Libraries:</p>
<ul>
<li><a href="https://github.com/facebookresearch/fastText">fastText by Facebook</a> - for efficient learning of word representations and sentence classification</li>
<li><a href="http://arxiv.org/pdf/1201.0490.pdf">Scikit-learn: Machine learning in Python</a></li>
<li><a href="http://www.nltk.org/">Natural Language Toolkit (NLTK)</a></li>
<li><a href="http://www.clips.ua.ac.be/pattern">Pattern</a> - A web mining module for the Python programming language. It has tools for natural language processing, machine learning, among others.</li>
<li><a href="http://textblob.readthedocs.org/">TextBlob</a> - Providing a consistent API for diving into common natural language processing (NLP) tasks. Stands on the giant shoulders of NLTK and Pattern, and plays nicely with both.</li>
<li><a href="https://github.com/machinalis/yalign">YAlign</a> - A sentence aligner, a friendly tool for extracting parallel sentences from comparable corpora.</li>
<li><a href="https://github.com/fxsjy/jieba#jieba-1">jieba</a> - Chinese Words Segmentation Utilities.</li>
<li><a href="https://github.com/isnowfy/snownlp">SnowNLP</a> - A library for processing Chinese text.</li>
<li><a href="http://konlpy.org">KoNLPy</a> - A Python package for Korean natural language processing.</li>
<li><a href="https://github.com/columbia-applied-data-science/rosetta">Rosetta</a> - Text processing tools and wrappers (e.g. Vowpal Wabbit)</li>
<li><a href="https://pypi.python.org/pypi/bllipparser/">BLLIP Parser</a> - Python bindings for the BLLIP Natural Language Parser (also known as the Charniak-Johnson parser)</li>
<li><a href="https://github.com/proycon/pynlpl">PyNLPl</a> - Python Natural Language Processing Library. General purpose NLP library for Python. Also contains some specific modules for parsing common NLP formats, most notably for <a href="http://proycon.github.io/folia/">FoLiA</a>, but also ARPA language models, Moses phrasetables, GIZA++ alignments.</li>
<li><a href="https://github.com/proycon/python-ucto">python-ucto</a> - Python binding to ucto (a unicode-aware rule-based tokenizer for various languages)</li>
<li><a href="https://github.com/datamade/parserator">Parserator</a> - A toolkit for making domain-specific probabilistic parsers</li>
<li><a href="https://github.com/proycon/python-frog">python-frog</a> - Python binding to Frog, an NLP suite for Dutch. (pos tagging, lemmatisation, dependency parsing, NER)</li>
<li><a href="https://github.com/EducationalTestingService/python-zpar">python-zpar</a> - Python bindings for <a href="https://github.com/frcchang/zpar">ZPar</a>, a statistical part-of-speech-tagger, constiuency parser, and dependency parser for English.</li>
<li><a href="https://github.com/proycon/colibri-core">colibri-core</a> - Python binding to C++ library for extracting and working with with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.</li>
<li><a href="https://github.com/spacy-io/spaCy">spaCy</a> - Industrial strength NLP with Python and Cython.</li>
<li><a href="https://github.com/chartbeat-labs/textacy">textacy</a> - Higher level NLP built on spaCy</li>
<li><a href="https://github.com/dmcc/PyStanfordDependencies">PyStanfordDependencies</a> - Python interface for converting Penn Treebank trees to Stanford Dependencies.</li>
<li><a href="https://radimrehurek.com/gensim/index.html">gensim</a> - Python library to conduct unsupervised semantic modelling from plain text</li>
<li><a href="https://github.com/JasonKessler/scattertext">scattertext</a> - Python library to produce d3 visualizations of how language differs between corpora.</li>
<li><a href="https://github.com/CogComp/cogcomp-nlpy">CogComp-NlPy</a> - Light-weight Python NLP annotators.</li>
<li><a href="https://github.com/wannaphongcom/pythainlp">PyThaiNLP</a> - Thai NLP in Python Package.</li>
<li><a href="https://github.com/datquocnguyen/jPTDP">jPTDP</a> - A toolkit for joint part-of-speech (POS) tagging and dependency parsing. jPTDP provides pre-trained models for 40+ languages.</li>
<li><a href="https://github.com/cltk/cltk">CLTK</a>: The Classical Language Toolkit is a Python library and collection of texts for doing NLP in ancient languages.</li>
<li><a href="https://github.com/kmike/pymorphy2">pymorphy2</a> - a good pos-tagger for Russian</li>
<li><a href="https://github.com/bigartm/bigartm">BigARTM</a> - a fast library for topic modelling</li>
<li><a href="https://github.com/allenai/allennlp">AllenNLP</a> - An NLP research library, built on PyTorch, for developing state-of-the-art deep learning models on a wide variety of linguistic tasks.</li>
</ul>

	<p></p>
	<p></p>
	<p></p>

	<p>C++ Libraries:</p>
<ul>
<li><a href="https://github.com/mit-nlp/MITIE">MIT Information Extraction Toolkit</a> - C, C++, and Python tools for named entity recognition and relation extraction</li>
<li><a href="https://taku910.github.io/crfpp/">CRF++</a> - Open source implementation of Conditional Random Fields (CRFs) for segmenting/labeling sequential data &amp; other Natural Language Processing tasks.</li>
<li><a href="http://www.chokkan.org/software/crfsuite/">CRFsuite</a> - CRFsuite is an implementation of Conditional Random Fields (CRFs) for labeling sequential data.</li>
<li><a href="https://github.com/BLLIP/bllip-parser">BLLIP Parser</a> - BLLIP Natural Language Parser (also known as the Charniak-Johnson parser)</li>
<li><a href="https://github.com/proycon/colibri-core">colibri-core</a> - C++ library, command line tools, and Python binding for extracting and working with basic linguistic constructions such as n-grams and skipgrams in a quick and memory-efficient way.</li>
<li><a href="https://github.com/LanguageMachines/ucto">ucto</a> - Unicode-aware regular-expression based tokenizer for various languages. Tool and C++ library. Supports FoLiA format.</li>
<li><a href="https://github.com/LanguageMachines/libfolia">libfolia</a> - C++ library for the <a href="http://proycon.github.io/folia/">FoLiA format</a></li>
<li><a href="https://github.com/LanguageMachines/frog">frog</a> - Memory-based NLP suite developed for Dutch: PoS tagger, lemmatiser, dependency parser, NER, shallow parser, morphological analyzer.</li>
<li><a href="https://github.com/meta-toolkit/meta">MeTA</a> - <a href="https://meta-toolkit.org/">MeTA : ModErn Text Analysis</a> is a C++ Data Sciences Toolkit that facilitates mining big text data.</li>
<li><a href="http://taku910.github.io/mecab/">Mecab (Japanese)</a></li>
<li><a href="http://eunjeon.blogspot.com/">Mecab (Korean)</a></li>
<li><a href="http://statmt.org/moses/">Moses</a></li>
<li><a href="https://github.com/facebookresearch/StarSpace">StarSpace</a> - a library from Facebook for creating embeddings of word-level, paragraph-level, document-level and for text classification</li>
</ul>

	<p></p>
	<p></p>
	<p></p>

	<p>Java Libraries:</p>
<ul>
<li><a href="http://nlp.stanford.edu/software/index.shtml">Stanford NLP</a></li>
<li><a href="http://opennlp.apache.org/">OpenNLP</a></li>
<li><a href="https://github.com/clir/clearnlp">ClearNLP</a></li>
<li><a href="http://deeplearning4j.org/word2vec.html">Word2vec in Java</a></li>
<li><a href="https://github.com/knowitall/reverb/">ReVerb</a> Web-Scale Open Information Extraction</li>
<li><a href="https://github.com/knowitall/openregex">OpenRegex</a> An efficient and flexible token-based regular expression language and engine.</li>
<li><a href="https://github.com/CogComp/cogcomp-nlp">CogcompNLP</a> - Core libraries developed in the U of Illinois' Cognitive Computation Group.</li>
<li><a href="http://mallet.cs.umass.edu/">MALLET</a> - MAchine Learning for LanguagE Toolkit - package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text.</li>
<li><a href="https://github.com/datquocnguyen/RDRPOSTagger">RDRPOSTagger</a> - A robust POS tagging toolkit available  (in both Java &amp; Python) together with pre-trained models for 40+ languages.</li>
</ul>

	<p></p>
	<p></p>
	<p></p>

	<p>Scala Libraries:</p>
<ul>
<li><a href="https://github.com/CogComp/saul">Saul</a> - Library for developing NLP systems, including built in modules like SRL, POS, etc.</li>
<li><a href="https://github.com/ispras/atr4s">ATR4S</a> - Toolkit with state-of-the-art <a href="https://en.wikipedia.org/wiki/Terminology_extraction">automatic term recognition</a> methods.</li>
<li><a href="https://github.com/ispras/tm">tm</a> - Implementation of topic modeling based on regularized multilingual <a href="https://en.wikipedia.org/wiki/Probabilistic_latent_semantic_analysis">PLSA</a>.</li>
<li><a href="https://github.com/Refefer/word2vec-scala">word2vec-scala</a> - Scala interface to word2vec model; includes operations on vectors like word-distance and word-analogy.</li>
<li><a href="https://github.com/dlwh/epic">Epic</a> - Epic is a high performance statistical parser written in Scala, along with a framework for building complex structured prediction models.</li>
</ul>

	<p></p>
	<p></p>
	<p></p>

	<p>R Libraries:</p>
<ul>
<li><a href="https://github.com/dselivanov/text2vec">text2vec</a> - Fast vectorization, topic modeling, distances and GloVe word embeddings in R.</li>
<li><a href="https://github.com/bmschmidt/wordVectors">wordVectors</a> - An R package for creating and exploring word2vec and other word embedding models</li>
<li><a href="https://github.com/mimno/RMallet">RMallet</a> - R package to interface with the Java machine learning tool MALLET</li>
<li><a href="https://github.com/agoldst/dfr-browser">dfr-browser</a> - Creates d3 visualizations for browsing topic models of text in a web browser.</li>
<li><a href="https://github.com/agoldst/dfrtopics">dfrtopics</a> - R package for exploring topic models of text.</li>
<li><a href="https://github.com/kevincobain2000/sentiment_classifier">sentiment_classifier</a> - Sentiment Classification using Word Sense Disambiguation and WordNet Reader</li>
<li><a href="https://github.com/kevincobain2000/jProcessing">jProcessing</a> - Japanese Natural Langauge Processing Libraries, with Japanese sentiment classification</li>
</ul>

	<p></p>
	<p></p>
	<p></p>

	<p>Clojure:</p>
<ul>
<li><a href="https://github.com/dakrone/clojure-opennlp">Clojure-openNLP</a> - Natural Language Processing in Clojure (opennlp)</li>
<li><a href="https://github.com/r0man/inflections-clj">Infections-clj</a> - Rails-like inflection library for Clojure and ClojureScript</li>
<li><a href="https://github.com/turbopape/postagga">postagga</a> - A library to parse natural language in Clojure and ClojureScript</li>
</ul>


	<p></p>
	<p></p>
	<p></p>

	<p>Ruby:</p>
<ul>
<li>Kevin Dias's <a href="https://github.com/diasks2/ruby-nlp">A collection of Natural Language Processing (NLP) Ruby libraries, tools and software</a></li>
<li><a href="https://github.com/arbox/nlp-with-ruby">Practical Natural Language Processing done in Ruby</a></li>
</ul>

	<p></p>
	<p></p>
	<p></p>

	<p>Rust:</p>
<ul>
<li><a href="https://github.com/greyblake/whatlang-rs">whatlang</a> — Natural language recognition library based on trigrams</li>
</ul>

	<p></p>
	<p></p>
	<p></p>

	<p>Services:</p>
<ul>
<li><a href="https://github.com/wit-ai/wit">Wit-ai</a> - Natural Language Interface for apps and devices</li>
<li>IBM Watson's <a href="https://github.com/watson-developer-cloud/natural-language-understanding-nodejs">Natural Language Understanding</a>, <a href="https://github.com/watson-developer-cloud/natural-language-classifier-nodejs">Natural Language Classifier</a> and <a href="https://github.com/watson-developer-cloud/language-translator-nodejs">Machine Translation</a> API Demos</li>
</ul>

	<p></p>
	<p></p>
	<p></p>

	<p>Articles:</p>
<ul>
<li><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/wsdm2015.v3.pdf">Deep Learning for Web Search and Natural Language Processing</a></li>
<li><a href="https://www.cs.princeton.edu/%7Eblei/papers/Blei2012.pdf">Probabilistic topic models</a></li>
<li><a href="http://jamia.oxfordjournals.org/content/18/5/544.short">Natural language processing: an introduction</a></li>
<li><a href="http://arxiv.org/pdf/1201.0490.pdf">A unified architecture for natural language processing: Deep neural networks with multitask learning</a></li>
<li><a href="http://arxiv.org/pdf/1506.00019v1.pdf">A Critical Review of Recurrent Neural Networksfor Sequence Learning</a></li>
<li><a href="http://nlp.cs.rpi.edu/course/spring14/deepparsing.pdf">Deep parsing in Watson</a></li>
<li><a href="http://arxiv.org/pdf/1301.2857.pdf">Online named entity recognition method for microtexts in social networking services: A case study of twitter</a></li>
</ul>
	<p></p>
	<p></p>
	<p>Word Vectors:</p>
<p>Resources about word vectors, aka word embeddings, and distributed representations for words.
Word vectors are numeric representations of words that are often used as input to deep learning systems. This process is sometimes called pretraining.</p>
<p><a href="http://arxiv.org/pdf/1301.3781v3.pdf">Efficient Estimation of Word Representations in Vector Space</a>
[Distributed Representations of Words and Phrases and their Compositionality]
(<a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a>)
<a href="https://scholar.google.com/citations?user=oBu8kMMAAAAJ&amp;hl=en">Mikolov</a> et al. 2013.
Generate word and phrase vectors.  Performs well on word similarity and analogy task and includes <a href="https://code.google.com/p/word2vec/">Word2Vec source code</a>  Subsamples frequent words. (i.e. frequent words like "the" are skipped periodically to speed things up and improve vector for less frequently used words
<a href="http://tensorflow.org/tutorials/word2vec/index.html">Word2Vec tutorial</a> in <a href="http://tensorflow.org/">TensorFlow</a></p>
<p><a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">Deep Learning, NLP, and Representations</a>
Chris Olah (2014)  Blog post explaining word2vec.</p>
<p><a href="http://nlp.stanford.edu/projects/glove/glove.pdf">GloVe: Global vectors for word representation</a>
Pennington, Socher, Manning. 2014. Creates word vectors and relates word2vec to matrix factorizations.  <a href="http://rare-technologies.com/making-sense-of-word2vec/">Evalutaion section led to controversy</a> by <a href="https://plus.google.com/114479713299850783539/posts/BYvhAbgG8T2">Yoav Goldberg</a>
<a href="http://nlp.stanford.edu/projects/glove/">Glove source code and training data</a></p>
<ul>
<li><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">word2vec</a> - on creating vectors to represent language, useful for RNN inputs</li>
<li><a href="http://arxiv.org/abs/1511.06388">sense2vec</a> - on word sense disambiguation</li>
<li><a href="http://arxiv.org/abs/1511.05392">Infinite Dimensional Word Embeddings</a> - new</li>
<li><a href="http://arxiv.org/abs/1506.06726">Skip Thought Vectors</a> - word representation method</li>
<li><a href="http://arxiv.org/abs/1502.07257">Adaptive skip-gram</a> - similar approach, with adaptive properties</li>
</ul>
	<p></p>
	<p></p>
	<p>Thought Vectors:</p>
<p>Thought vectors are numeric representations for sentences, paragraphs, and documents.  The following papers are listed in order of date published, each one replaces the last as the state of the art in sentiment analysis.</p>
<p><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.383.1327&amp;rep=rep1&amp;type=pdf">Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank</a>
Socher et al. 2013.  Introduces Recursive Neural Tensor Network.  Uses a parse tree.</p>
<p><a href="http://cs.stanford.edu/%7Equocle/paragraph_vector.pdf">Distributed Representations of Sentences and Documents</a>
<a href="https://scholar.google.com/citations?user=vfT6-XIAAAAJ">Le</a>, Mikolov. 2014.  Introduces Paragraph Vector. Concatenates and averages pretrained, fixed word vectors to create vectors for sentences, paragraphs and documents. Also known as paragraph2vec.  Doesn't use a parse tree.
Implemented in <a href="https://github.com/piskvorky/gensim/">gensim</a>.  See <a href="http://rare-technologies.com/doc2vec-tutorial/">doc2vec tutorial</a></p>
<p><a href="http://www.cs.cornell.edu/%7Eoirsoy/files/nips14drsv.pdf">Deep Recursive Neural Networks for Compositionality in Language</a>
Irsoy &amp; Cardie. 2014.  Uses Deep Recursive Neural Networks. Uses a parse tree.</p>
<p><a href="https://aclweb.org/anthology/P/P15/P15-1150.pdf">Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks</a>
Tai et al. 2015  Introduces Tree LSTM. Uses a parse tree.</p>
<p><a href="http://arxiv.org/pdf/1511.01432.pdf">Semi-supervised Sequence Learning</a>
Dai, Le 2015 "With pretraining, we are able to train long short term memory recurrent networks up to a few hundred
timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups."</p>
	<p></p>
	<p></p>
	<p>Machine Translation:</p>
<p><a href="http://arxiv.org/pdf/1409.0473v6.pdf">Neural Machine Translation by jointly learning to align and translate</a>
Bahdanau, Cho 2014.  "comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation."  Implements attention mechanism.
<a href="http://104.131.78.120/">English to French Demo</a></p>
<p><a href="http://arxiv.org/pdf/1409.3215v3.pdf">Sequence to Sequence Learning with Neural Networks</a>
Sutskever, Vinyals, Le 2014.  (<a href="http://research.microsoft.com/apps/video/?id=239083">nips presentation</a>). Uses LSTM RNNs to generate translations. " Our main result is that on an English to French translation task from the WMT’14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8"
<a href="http://tensorflow.org/tutorials/seq2seq/index.html">seq2seq tutorial</a> in</p>
<ul>
<li><a href="http://arxiv.org/pdf/1310.1597v1.pdf">Cross-lingual Pseudo-Projected Expectation Regularization for Weakly Supervised Learning</a></li>
<li><a href="http://www.mt-archive.info/IJCNLP-2011-Fu.pdf">Generating Chinese Named Entity Data from a Parallel Corpus</a></li>
<li><a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/775_Paper.pdf">IXA pipeline: Efficient and Ready to Use Multilingual NLP tools</a></li>
</ul>
	<p></p>
	<p></p>
	<p>Single Exchange Dialogs:</p>
<p><a href="http://arxiv.org/pdf/1506.06714v1.pdf">A Neural Network Approach toContext-Sensitive Generation of Conversational Responses</a>
Sordoni 2015.  Generates responses to tweets.
Uses <a href="http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf">Recurrent Neural Network Language Model (RLM) architecture
of (Mikolov et al., 2010).</a>  source code: <a href="http://www.rnnlm.org/">RNNLM Toolkit</a></p>
<p><a href="http://arxiv.org/pdf/1503.02364v2.pdf">Neural Responding Machine for Short-Text Conversation</a>
Shang et al. 2015  Uses Neural Responding Machine.  Trained on Weibo dataset.  Achieves one round conversations with 75% appropriate responses.</p>
<p><a href="http://arxiv.org/pdf/1506.05869v3.pdf">A Neural Conversation Model</a>
Vinyals, <a href="https://scholar.google.com/citations?user=vfT6-XIAAAAJ">Le</a> 2015.  Uses LSTM RNNs to generate conversational responses. Uses <a href="http://tensorflow.org/tutorials/seq2seq/index.html">seq2seq framework</a>.  Seq2Seq was originally designed for machine transation and it "translates" a single sentence, up to around 79 words, to a single sentence response, and has no memory of previous dialog exchanges.  Used in Google <a href="http://googleresearch.blogspot.co.uk/2015/11/computer-respond-to-this-email.html">Smart Reply feature for Inbox</a></p>
	<p></p>
	<p></p>
	<p>Memory and Attention Models (from DL4NLP):</p>
<p><a href="http://www.thespermwhale.com/jaseweston/ram/">Reasoning, Attention and Memory RAM workshop at NIPS 2015. slides included</a></p>
<p><a href="http://arxiv.org/pdf/1410.3916v10.pdf">Memory Networks</a> Weston et. al 2014, and
<a href="http://arxiv.org/pdf/1503.08895v4.pdf">End-To-End Memory Networks</a> Sukhbaatar et. al 2015.
Memory networks are implemented in <a href="https://github.com/facebook/MemNN">MemNN</a>.  Attempts to solve task of reason attention and memory.
<a href="http://arxiv.org/pdf/1502.05698v7.pdf">Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</a>
Weston 2015. Classifies QA tasks like single factoid, yes/no etc. Extends memory networks.
<a href="http://arxiv.org/pdf/1511.06931.pdf">Evaluating prerequisite qualities for learning end to end dialog systems</a>
Dodge et. al 2015. Tests Memory Networks on 4 tasks including reddit dialog task.
See <a href="https://www.youtube.com/watch?v=Xumy3Yjq4zk">Jason Weston lecture on MemNN</a></p>
<p><a href="http://arxiv.org/pdf/1410.5401v2.pdf">Neural Turing Machines</a>
Graves et al. 2014.</p>
<p><a href="http://arxiv.org/pdf/1503.01007v4.pdf">Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets</a>
Joulin, Mikolov 2015. <a href="https://github.com/facebook/Stack-RNN">Stack RNN source code</a> and <a href="https://research.facebook.com/blog/1642778845966521/inferring-algorithmic-patterns-with-stack/">blog post</a></p>
	<p></p>
	<p></p>
	<p>General Natural Language Processing:</p>
<ul>
<li><a href="http://arxiv.org/abs/1506.01057">Neural autocoder for paragraphs and documents</a> - LSTM representation</li>
<li><a href="http://arxiv.org/abs/1503.04881">LSTM over tree structures</a></li>
<li><a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sequence to Sequence Learning</a> - word vectors for machine translation</li>
<li><a href="http://arxiv.org/abs/1506.03340">Teaching Machines to Read and Comprehend</a> - DeepMind paper</li>
<li><a href="http://arxiv.org/pdf/1301.3781.pdf">Efficient Estimation of Word Representations in Vector Space</a></li>
<li><a href="https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/viewFile/570/124">Improving distributional similarity with lessons learned from word embeddings</a></li>
<li><a href="http://www.aclweb.org/anthology/W/W14/W14-2409.pdf">Low-Dimensional Embeddings of Logic</a></li>
<li>Tutorial on Markov Logic Networks (<a href="http://homes.cs.washington.edu/%7Epedrod/papers/mlj05.pdf">based on this paper</a>)</li>
<li><a href="http://arxiv.org/pdf/1507.03045v1.pdf">Markov Logic Networks for Natural Language Question Answering</a></li>
<li><a href="http://research.microsoft.com/en-us/um/people/hoifung/papers/psb15.pdf">Distant Supervision for Cancer Pathway Extraction From Text</a></li>
<li><a href="http://www.sebastianzimmeck.de/zimmeckAndBellovin2014Privee.pdf">Privee: An Architecture for Automatically Analyzing Web Privacy Policies</a></li>
<li><a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a></li>
<li><a href="http://www.usna.edu/Users/cs/nchamber/pubs/acl2011-chambers-templates.pdf">Template-Based Information Extraction without the Templates</a></li>
<li><a href="http://www.cs.cmu.edu/%7Emfaruqui/papers/naacl15-retrofitting.pdf">Retrofitting word vectors to semantic lexicons</a></li>
<li><a href="http://www.mitpressjournals.org/doi/pdfplus/10.1162/089120101750300490">Unsupervised Learning of the Morphology of a Natural Language</a></li>
<li><a href="http://arxiv.org/pdf/1103.0398.pdf">Natural Language Processing (Almost) from Scratch</a></li>
<li><a href="http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00612/full">Computational Grounded Cognition: a new alliance between grounded cognition and computational modelling</a></li>
<li><a href="http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004216">Learning the Structure of Biomedical Relation Extractions</a></li>
<li><a href="http://www.anthology.aclweb.org/N/N13/N13-1008.pdf">Relation extraction with matrix factorization and universal schemas</a></li>
</ul>
	<p></p>
	<p></p>
	<p>Named Entity Recognition:</p>
<ul>
<li><a href="http://nlp.cs.nyu.edu/sekine/papers/li07.pdf">A survey of named entity recognition and classification</a></li>
<li><a href="http://www.lrec-conf.org/proceedings/lrec2014/pdf/176_Paper.pdf">Benchmarking the extraction and disambiguation of named entities on the semantic web</a></li>
<li><a href="http://www.aclweb.org/anthology/P11-1115">Knowledge base population: Successful approaches and challenges</a></li>
<li><a href="http://arxiv.org/pdf/1301.2857.pdf">SpeedRead: A fast named entity recognition Pipeline</a></li>
</ul>
	<p></p>
	<p></p>
	<p>Neural Networks:</p>
<li><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
<li><a href="http://www.fit.vutbr.cz/%7Eimikolov/rnnlm/thesis.pdf">Statistical Language Models based on Neural Networks</a></li>
<li><a href="http://www.fit.vutbr.cz/%7Eimikolov/rnnlm/google.pdf">Slides from Google Talk</a></li>
</ul>
	<p></p>
	<p></p>
	<p>Supplementary Materials:</p>
<ul>
<li><a href="https://github.com/facebookresearch/DrQA">DrQA: Open Domain Question Answering</a> by facebook on Wikipedia data</li>
<li><a href="https://github.com/clulab/nlp-reading-group/wiki/Word2Vec-Resources">Word2Vec</a></li>
<li><a href="http://www.riedelcastro.org//publications/papers/riedel13relation.pdf">Relation Extraction with Matrix Factorization and Universal Schemas</a></li>
<li><a href="http://www.aclweb.org/anthology/S13-1001">Towards a Formal Distributional Semantics: Simulating Logical Calculi with Tensors</a></li>
<li><a href="https://github.com/clulab/nlp-reading-group/blob/master/fall-2015-resources/mln-summary-20150918.ppt">Presentation slides for MLN tutorial</a></li>
<li><a href="https://github.com/clulab/nlp-reading-group/blob/master/fall-2015-resources/Markov%20Logic%20Networks%20for%20Natural%20Language%20Question%20Answering.pdf">Presentation slides for QA applications of MLNs</a></li>
<li><a href="https://github.com/clulab/nlp-reading-group/blob/master/fall-2015-resources/poon-paper.pdf">Presentation slides</a></li>
<li><a href="https://homes.cs.washington.edu/%7Eclzhang/paper/acl2011.pdf">Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations</a></li>
</ul>
	<p></p>
	<p></p>
	<p>Blogs:</p>
<ul>
<li>Blog Post on <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">Deep Learning, NLP, and Representations</a></li>
<li>Blog Post on <a href="http://www.vikparuchuri.com/blog/natural-language-processing-tutorial/">NLP Tutorial</a></li>
<li><a href="http://nlpers.blogspot.ch/">Natural Language Processing Blog</a> by Hal Daumé III</li>
<li><a href="https://bmcfee.github.io/#home">Machine Learning Blog</a> by Brian McFee</li>
<li>Ultimate Guide to <a href="https://www.analyticsvidhya.com/blog/2017/01/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python/">Understand &amp; Implement Natural Language Processing</a></li>
<li><a href="http://aiplaybook.a16z.com/">AI Playbook</a>: An easily understandable introductory website.
*[Introduction to NLP]
(<a href="https://hackernoon.com/learning-ai-if-you-suck-at-math-p7-the-magic-of-natural-language-processing-f3819a689386">https://hackernoon.com/learning-ai-if-you-suck-at-math-p7-the-magic-of-natural-language-processing-f3819a689386</a>)</li>
</ul>
	<p></p>
	<p></p>
	<p>Credits:</p>
<ul>
<li><a href="https://github.com/m0nologuer/AI-reading-list">ai-reading-list</a></li>
<li><a href="https://github.com/clulab/nlp-reading-group/wiki/Fall-2015-Reading-Schedule/_edit">nlp-reading-group</a></li>
<li><a href="https://github.com/dav009/awesome-spanish-nlp">awesome-spanish-nlp</a></li>
<li><a href="https://gist.github.com/jjangsangy/8759f163bc3558779c46">jjangsangy's awesome-nlp</a></li>
<li><a href="https://github.com/josephmisiti/awesome-machine-learning/edit/master/README.md">awesome-machine-learning</a></li>
<li><a href="https://github.com/andrewt3000/DL4NLP">DL4NLP</a></li>
</ul>

	<p></p>
	<p></p>
	<p>Tutorials and Courses:</p>


<ul>
<li>Tensor Flow Tutorial on <a href="https://www.tensorflow.org/tutorials/seq2seq/index.html">Seq2Seq</a> Models</li>
<li>Natural Language Understanding with Distributed Representation <a href="https://github.com/nyu-dl/NLP_DL_Lecture_Note">Lecture Note</a> by Cho</li>
<li><a href="http://www.cs.columbia.edu/%7Emcollins/">Michael Collins</a> - one of the best NLP teachers. Check out the material on the courses he is teaching.</li>
<li>Several <a href="https://radimrehurek.com/gensim/tutorial.html">tutorials by Radim Řehůřek</a> on using Python and <a href="https://radimrehurek.com/gensim/index.html">gensim</a> to process corpora and conduct Latent Semantic Analysis and Latent Dirichlet Allocation experiments.</li>
<li><a href="https://www.manning.com/books/natural-language-processing-in-action">Natural Language Processing in Action</a> - A guide to creating machines that understand human language.</li>
</ul>
<p></p>
<p></p>
<p>Videos:</p>
<ul>
<li><a href="https://www.coursera.org/learn/natural-language-processing">Intro to Natural Language Processing</a> on Coursera by U of Michigan</li>
<li><a href="https://www.udacity.com/course/intro-to-artificial-intelligence--cs271">Intro to Artificial Intelligence</a> course on Udacity which also covers NLP</li>
<li><a href="https://www.youtube.com/playlist?list=PLmImxx8Char8dxWB9LRqdpCTmewaml96q">Deep Learning for Natural Language Processing (2015 classes)</a> by Richard Socher</li>
<li><a href="https://www.youtube.com/playlist?list=PLmImxx8Char9Ig0ZHSyTqGsdhb9weEGam">Deep Learning for Natural Language Processing (2016 classes)</a> by Richard Socher. Updated to make use of Tensorflow. Note that there are some lectures missing (lecture 9, and lectures 12 onwards).</li>
<li><a href="https://www.coursera.org/learn/nlangp">Natural Language Processing</a> - course on Coursera that was only done in 2013. The videos are not available at the moment. Also Mike Collins is a great professor and his notes and lectures are very good.</li>
<li><a href="http://mt-class.org">Statistical Machine Translation</a> - a Machine Translation course with great assignments and slides.</li>
<li><a href="http://www.cs.sfu.ca/%7Eanoop/teaching/CMPT-413-Spring-2014/">Natural Language Processing SFU - Link is broken</a> - course by <a href="https://www.cs.sfu.ca/%7Eanoop/">Prof Anoop Sarkar</a> on Natural Language Processing. Good notes and some good lectures on youtube about HMM.</li>
<li><a href="https://classroom.udacity.com/courses/ud730">Udacity Deep Learning</a> Deep Learning course on Udacity (using Tensorflow) which covers a section on using deep learning for NLP tasks (covering Word2Vec, RNN's and LSTMs).</li>
<li><a href="https://www.youtube.com/playlist?list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL">NLTK with Python 3 for Natural Language Processing</a> by Harrison Kinsley(sentdex). Good tutorials with NLTK code implementation.</li>
<li><a href="https://www.youtube.com/playlist?list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL">Computational Linguistics I</a> by Jordan Boyd-Graber . Lectures from University of Maryland.</li>
<li><a href="https://www.youtube.com/playlist?list=PL6397E4B26D00A269">Natural Language Processing - Stanford</a> by Dan Jurafsky &amp; Chris Manning. Previously on coursera. <a href="http://www.mohamedaly.info/teaching/cmp-462-spring-2013">Lecture Notes</a></li>
</ul>
	<p></p>
	<p></p>
	<p>Deep Learning for NLP:</p>
<p><a href="https://github.com/oxford-cs-deepnlp-2017/lectures">Deep Natural Language Processing</a>
+lecture slides and course description for the Deep Natural Language Processing course offered in Hilary Term 2017 at the University of Oxford.
<a href="http://cs224d.stanford.edu/syllabus.html">Stanford CS 224D: Deep Learning for NLP class</a>
Class by <a href="https://scholar.google.com/citations?user=FaOcyfMAAAAJ&amp;hl=en">Richard Socher</a>. 2016 content was updated to make use of Tensorflow. Lecture slides and reading materials for 2016 class <a href="http://cs224d.stanford.edu/syllabus.html">here</a>. Videos for 2016 class <a href="https://www.youtube.com/playlist?list=PLmImxx8Char9Ig0ZHSyTqGsdhb9weEGam">here</a>. Note that there are some lecture videos missing for 2016 (lecture 9, and lectures 12 onwards). All videos for 2015 class <a href="https://www.youtube.com/playlist?list=PLmImxx8Char8dxWB9LRqdpCTmewaml96q">here</a></p>
<p><a href="https://classroom.udacity.com/courses/ud730">Udacity Deep Learning</a>
Deep Learning course on Udacity (using Tensorflow) which covers a section on using deep learning for NLP tasks. This section covers how to implement Word2Vec, RNN's and LSTMs.</p>
<p><a href="http://u.cs.biu.ac.il/%7Eyogo/nnlp.pdf">A Primer on Neural Network Models for Natural Language Processing</a>
Yoav Goldberg. October 2015. No new info, 75 page summary of state of the art.</p>
    </div>
  </div>
</div>



<!-- Footer -->
<footer class="w3-container w3-padding-64 w3-center w3-opacity">  
  <div class="w3-xlarge w3-padding-32">
    <i class="fa fa-github w3-hover-opacity"></i>
    <i class="fa fa-twitter w3-hover-opacity"></i>
    <i class="fa fa-linkedin w3-hover-opacity"></i>
 </div>
 <p>Powered by <a href="https://www.w3schools.com/w3css/default.asp" target="_blank">w3.css</a></p>
</footer>

<script>
// Used to toggle the menu on small screens when clicking on the menu button
function myFunction() {
    var x = document.getElementById("navDemo");
    if (x.className.indexOf("w3-show") == -1) {
        x.className += " w3-show";
    } else { 
        x.className = x.className.replace(" w3-show", "");
    }
}
</script>

</body>
</html>
